---

layout: post
title: k8s批处理调度
category: 架构
tags: Kubernetes
keywords: kube-batch,volcano
---

## 简介

* TOC
{:toc}

[kube-batch](https://github.com/kubernetes-sigs/kube-batch)

如果一个podGroup minMember=10，且10个pod 正在运行，如果一个pod 失败，gang-scheduler 会有反应嘛？未完成。

## 必要性

[kube-batch在AI计算平台的应用](https://mp.weixin.qq.com/s/zXiSC0RWmow8RJ7XLog8JQ)k8s原生的调度器，会将需要启动的容器，放到一个优先队列（Priority Queue）里面，每次从队列里面取出一个容器，将其调度到一个节点上。 分布式训练需要所有worker都启动后，训练才能够开始进行。使用原生调度器，可能会出现以下问题：        
1. 一个任务包含了10个worker, 但是集群的资源只满足9个worker。原生调度器会将任务的9个worker调度并启动，而最后一个worker一直无法启动。这样训练一直无法开始，9个已经启动的worker的资源被浪费了。
2. 两个任务，各包含10个worker, 集群的资源只能启动10个worker。两个任务分别有5个worker被启动了，但两个任务都无法开始训练。10个worker的资源被浪费了。
3. 因为运行一个胖业务，饿死大批小业务


## 示例文件

kube-batch 案例

```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: qj-1
spec:
  backoffLimit: 6
  completions: 6
  parallelism: 6
  template:
    metadata:
      annotations:
        scheduling.k8s.io/group-name: qj-1
    spec:
      containers:
      - image: busybox
        imagePullPolicy: IfNotPresent
        name: busybox
        resources:
          requests:
            cpu: "1"
      restartPolicy: Never
	  ## 使用 kube-batch调度器
      schedulerName: kube-batch 
---
apiVersion: scheduling.incubator.k8s.io/v1alpha1
kind: PodGroup
metadata:
  name: qj-1
spec:
  minMember: 6
```

## 基本理念

kube-batch 本身是一个是scheduler，从apiserver 获取pod信息，如果pod 的 schedulerName 不是kube-batch 就会ignore。

虽然我们使用kube-batch是为了gang-scheduler（至少笔者是这样），kube-batch 作为一个调度器，基本的“为pod 选择一个最合适的node/node间pod 数量尽可能均衡/抢占” 这些特性还是要支持的。因此在设计上，即便不需要 像default scheduler 那么灵活，至少在代码层面要方便扩展，方便塞入个性化的调度需求。

![](/public/upload/kubernetes/volcano_workflow.png)

### queue/podgroup

1. queue是容纳一组podgroup的队列，也是该组podgroup获取集群资源的划分依据。queue 是资源管理的基本单位（weight是软约束，capacity 是硬约束，reclaimable 是超出软约束占据资源后是不是可以吐出来），podGroup 是调度的基本单位
2. 当创建vcjob（Volcano Job的简称）时，若没有指定该vcjob所属的podgroup，默认会为该vcjob创建同名的podgroup。

![](/public/upload/kubernetes/volcano_queue.png)


能够将一个训练任务的多个worker当做一个整体进行调度，只有当任务所有worker的资源都满足，才会将容器在节点上启动；kube-batch还提供了队列的机制（其实就是**多租户**），不同队列之间可以设置优先级，优先级高的队列中的任务会优先得到调度。队列还可以设置权重，权重高的队列分配到的资源会更多。PS: 换个表述，将调度单元从 Pod 修改为 PodGroup，**以组的形式进行调度**。

### action 和plugin

Action 实现了调度机制（mechanism），Plugin 实现了调度的不同策略（policy）。

![](/public/upload/kubernetes/kube_batch_action_plugin.png)

action负责管理核心逻辑和流程，xxFns 是流程里暴露出来的hook，一个plugin（扩展需求）通过一个或多个Fns 组合来实现，这就很像default-scheduler 中的 Scheduling Framework 了。[Volcano火山：容器与批量计算的碰撞](https://bbs.huaweicloud.com/blogs/205045)action 和plugin 的关系
1. action是第一级插件，定义了调度周期内需要的各个动作；默认提供 enqueue、allocate、 preempt和backfill四个action。以allocate为例，它定义了调度中资源分配过程：根据 plugin 的 JobOrderFn 对作业进行排序，根据NodeOrderFn对节点进行排序，检测节点上的资源是否满足，满足作业的分配要求(JobReady)后提交分配决定。由于action也是基于插件机制，因此用户可以重新定义自己的分配动作，例如 基于图的调度算法firmament。
2. plugin是第二级插件，定义了action需要的各个算法；比如如何为job排序，为node排序

总体来讲，带有动作属性的功能，一般需要引入 action 插件；带有选择 (包括排序) 属性的功能，一般使用 plugin 插件。

Volcano Scheduler是负责Pod调度的组件，它由一系列action和plugin组成。action定义了调度各环节中需要执行的动作；plugin根据不同场景提供了action 中算法的具体实现细节。Volcano scheduler的工作流程如下：

1. 客户端提交的Job被scheduler观察到并缓存起来。
2. 周期性的开启会话，一个调度周期开始。
3. 将没有被调度的Job发送到会话的待调度队列中。
4. 遍历所有的待调度Job，按照定义的次序依次执行enqueue、allocate、preempt、reclaim、backfill等动作，为每个Job找到一个最合适的节点。将该Job 绑定到这个节点。action中执行的具体算法逻辑取决于注册的plugin中各函数的实现。
	1. Enqueue action负责通过一系列的过滤算法筛选出符合要求的待调度任务并将它们送入待调度队列。经过这个action，任务的状态将由pending变为inqueue。
	2. Allocate action负责通过一系列的预选和优选算法筛选出最适合的节点。删除可以牺牲的pod 
	3. Preempt action用于同一个Queue中job之间的抢占，或同一Job下Task之间的抢占。
	4. Reclaim action负责当一个新的任务进入待调度队列，但集群资源已不能满足该任务所在队列的要求时，根据队列权重回收队列应得资源。选择满足条件的pod 删除。
	5. backfill action负责将处于pending状态的任务尽可能的调度下去以保证节点资源的最大化利用。处理待调度Pod列表中没有指明资源申请量的Pod调度，在对单个Pod执行调度动作的时候，遍历所有的节点，只要节点满足了Pod的调度请求，就将Pod调度到这个节点上。在一个集群中，主要资源被“胖业务”占用，例如AI模型的训练。Backfill action让集群可以快速调度诸如单次AI模型识别、小数据量通信的“小作业” 。Backfill能够提高集群吞吐量，提高资源利用率。
5. 关闭本次会话（清理中间数据，有些plugin 记录一些数据）。



### pod 状态变化

pod 在k8s cluster、scheduler session（一个调度周期）、scheduler cache中的状态，目前Volcano调度器仅使用了状态的部分功能
1. Pending: 当Pod被创建后就处于Pending状态，等待调度器对其进行调度；调度的主要目的也是为这些Pending的Pod寻找最优的资源
2. Allocated: 当Pod被分配空闲资源，但是还没有向kube-apiserver发送调度决策时，Pod处于Allocated状态。 Allocated状态仅存在于调度周期内部，用于记录Pod和资源分配情况。当作业满足启动条件时 (e.g. 满足minMember)，会向kube-apiserver提交调度决策。如果本轮调度周期内无法提交调度决策，由状态会回滚为Pending状态。
3. Pipelined: 该状态与Allocated状态相似，区别在于处于该状态的Pod分配到的资源为正在被释放的资源 (Releasing)。该状态主要用于等待被抢占的资源释放。该状态是调度周期中的状态，不会更新到kube-apiserver以减少通信，节省kube-apiserver的qps。
4. Binding: 当作业满足启动条件时，调度器会向kube-apiserver提交调度决策，在kube-apiserver返回最终状态之前，Pod一直处于Binding状态。该状态也保存在调度器的Cache之中，因此**跨调度周期**有效。
5. Bound: 当作业的调度决策在kube-apiserver确认后，该Pod即为Bound状态。
6. Releasing: Pod等待被删除时即为Releasing状态。
7. Running, Failed, Succeeded, Unknown: 与Pod的现有含义一致。

![](/public/upload/kubernetes/podgroup_status_dag.png)


### session

由于在分布式系统中很难保证信息的同步，因此调度器经常以某一时间点的集群快照进行调度；在每个**调度周期**都会创建一个Session对象，用来存储当前调度周期的所需的数据，例如，Cache 的一个快照。当前的调度器中仅创建了一个Session，并由一个调度线程执行。PS：在一个调度周期，基于snapshot 数据，找到当前资源可以运行的最高优先级的pod，**全局决策**，也是批量调度的一个内涵。

plugin 会根据自己的语义 注册相关的函数到 Session中，在Action.Execute 中被调用。

```go
// kube-batch/pkg/scheduler/framework/session.go
type Session struct {
	UID types.UID
	cache cache.Cache
	Jobs    map[api.JobID]*api.JobInfo
	Nodes   map[string]*api.NodeInfo
	Queues  map[api.QueueID]*api.QueueInfo
	Backlog []*api.JobInfo
	Tiers   []conf.Tier
	plugins          map[string]Plugin
	eventHandlers    []*EventHandler
	jobOrderFns      map[string]api.CompareFn	// 对job 排序
	queueOrderFns    map[string]api.CompareFn	// 对queue 排序
	taskOrderFns     map[string]api.CompareFn	// task 排序
	predicateFns     map[string]api.PredicateFn		// 预选 
	preemptableFns   map[string]api.EvictableFn		// 优选
	reclaimableFns   map[string]api.EvictableFn		// 判断task 是否可以被回收
	overusedFns      map[string]api.ValidateFn		
	jobReadyFns      map[string]api.ValidateFn		// job 是否ready
	jobPipelinedFns  map[string]api.ValidateFn		
	jobValidFns      map[string]api.ValidateExFn
	nodePrioritizers map[string][]priorities.PriorityConfig
}
// kube-batch/pkg/scheduler/framework/session_plugins.go
func (ssn *Session) AddJobReadyFn(name string, vf api.ValidateFn) {...}
func (ssn *Session) JobReady(obj interface{}) bool {...jobValidFns...}
func (ssn *Session) AddJobValidFn(name string, fn api.ValidateExFn) {...}
func (ssn *Session) JobValid(obj interface{}) *api.ValidateResult {...jobReadyFns...}
// kube-batch/pkg/scheduler/framework/session.go
func (ssn *Session) Allocate(task *api.TaskInfo, hostname string) error {...}
func (ssn *Session) Pipeline(task *api.TaskInfo, hostname string) error {...}
func (ssn *Session) Evict(reclaimee *api.TaskInfo, reason string) error {...}
```
Action 只有一个Session 一个入参， **Session 一共有两类方法**：session_plugins，与plugin 相关的各种Function 注入与调用；真正操作Pod的Allocate/Pipeline/Evict。Action.Execute中，Action 依次遍历 pending 状态的task，根据session_plugins方法判断task 和job 状态，最终调用Pod的Allocate/Pipeline/Evict。这或许是Action 和Plugin ，机制和策略分离的一种解释。

### 调度策略

[调度策略汇总](https://volcano.sh/zh/docs/plugins/) 

||思路|优点|算法|
|---|---|---|---|
|Gang|满足了调度过程中的“All or nothing”的调度需求|避免Pod的任意调度导致集群资源的浪费|观察Job下的Pod已调度数量是否满足了最小运行数量，当Job的最小运行数量得到满足时，为Job下的所有Pod执行调度动作，否则，不执行|
|Binpack|binpack调度算法的目标是尽量把已有的节点填满（尽量不往空白节点分配）|尽可能填满节点的小作业有利，在空闲的机器上为申请了更大资源请求的Pod预留足够的资源空间|**Binpack算法以插件的形式，注入到volcano-scheduler调度过程中，将会应用在Pod优选节点的阶段**（这个表述对理解action和plugin的关系）|
|Priority|让用户自定义job、task优先级||
|DRF|具有较低share值的Job将具有更高的调度优先级|优先考虑集群中业务的吞吐量，适用单次AI训练、单次大数据计算以及查询等批处理小业务场景|
|Proportion|不同团队使用不同的Queue|
|Task-topology|根据Job内task之间亲和性和反亲和性配置计算task优先级和Node优先级的算法||node affinity/Anti-affinity，以TensorFlow计算为例，“ps”与“ps”之间的反亲和性|
|SLA|用户可以在自己的集群定制SLA相关参数，例如最长等待时间(JobWaitingTime)|
|Tdm|在特定的时间将任务调度到标记的节点上，其它时间则不调度|提高了volcano在调度过程中节点资源的分时复用能力|
|Numa-aware|许多工作负载对cpu资源迁移并不敏感。然而，有一些cpu的缓存亲和度以及调度延迟显著影响性能的工作负载|

## kube-batch

尝试以数据结构 + 算法的方式理解下，未完成。

```go
// kube-batch/pkg/scheduler/scheduler.go
func (pc *Scheduler) Run(stopCh <-chan struct{}) {
	// Start cache for policy.
	go pc.cache.Run(stopCh)
	pc.cache.WaitForCacheSync(stopCh)
	// Load configuration of scheduler
	pc.actions, pc.plugins, err = loadSchedulerConf(schedConf)
	go wait.Until(pc.runOnce, pc.schedulePeriod, stopCh)
}
func (pc *Scheduler) runOnce() {
	ssn := framework.OpenSession(pc.cache, pc.plugins)
	defer framework.CloseSession(ssn)
	for _, action := range pc.actions {
		actionStartTime := time.Now()
		action.Execute(ssn)
	}
}
```

### gang-scheduler

[kube-batch 具体如何实现gang scheduler](https://www.jianshu.com/p/0c191c4aeb5a)
[kube-batch 从代码中找出gang scheduler这个过程](https://www.jianshu.com/p/9c19e4bb061a)

gang-scheduler 非常类似分布式事务/tcc，tcc 有一个预留的动作，要实现gang-scheduler的效果，Pod 自带的Pending/Running/Succeeded/Failed/Unknown 是不够的， 为此Pod 对应struct TaskInfo 定义了Pending/Allocated/Pipelined/Binding/Bound/Running/Releasing/Succeeded/Failed/Unknown 状态，其中 Allocated 用来标记pod 已分配资源但未实际运行的状态。

当需要进行gang-scheduler  时，上层operator/controller 会将pod 的schedulerName 设置为kube-batch或volcano，并带上 annotation `scheduling.k8s.io/group-name`，创建 name= `scheduling.k8s.io/group-name` 的podgroup，即podgroup 和 pod 通过`scheduling.k8s.io/group-name` 关联。

一个podGroup 对应一个JobInfo，kube-batch 将pod 转换为taskInfo，每一个node对应NodeInfo，所谓 为pod分配Node：taskInfo.NodeName=nodeName，NodeInfo减去pod 标定的资源。当发现 JobInfo 下的taskInfo 符合minMember，即真正为 pod 赋值nodeName。具体代码还要再捋捋。

```go
func (alloc *allocateAction) Execute(ssn *framework.Session) {
	...
	// 对queue和job 进行排序  queues 和jobs 都是优先级队列
	for {
		if queues.Empty() {break}
		queue := queues.Pop().(*api.QueueInfo)	// 取出优先级最高的queue
		if ssn.Overused(queue) {continue} // 某个queue 占用的资源过多，不再为其pod进行调度了
		jobs, found := jobsMap[queue.UID]	 // 取出queue 对应的jobs
		job := jobs.Pop().(*api.JobInfo)
		// 赋值api.Pending状态的task 到  pendingTasks
		tasks := pendingTasks[job.UID]
		glog.V(3).Infof("Try to allocate resource to %d tasks of Job <%v/%v>",
		tasks.Len(), job.Namespace, job.Name)
		for !tasks.Empty() {
			task := tasks.Pop().(*api.TaskInfo)
			predicateNodes := util.PredicateNodes(task, allNodes, predicateFn)  // 预选
			if len(predicateNodes) == 0 {break} 
			priorityList, err := util.PrioritizeNodes(task, predicateNodes, ssn.NodePrioritizers()) // 优选
			if err != nil {break} 
			nodeName := util.SelectBestNode(priorityList)
			node := ssn.Nodes[nodeName]
			// Allocate idle resource to the task.
			if task.InitResreq.LessEqual(node.Idle) {
				if err := ssn.Allocate(task, node.Name); err != nil {...} // 绑定task到node
			} else {  
				//store information about missing resources
				job.NodesFitDelta[node.Name] = node.Idle.Clone()
				job.NodesFitDelta[node.Name].FitDelta(task.InitResreq)
				// Allocate releasing resource to the task if any.
				if task.InitResreq.LessEqual(node.Releasing) {
					if err := ssn.Pipeline(task, node.Name); err != nil {...}
				}
			}
			// job ready（比如job一共10个 minMember=5）当前job 放在jobs的最后，还剩的5的pod 调度优先级就不高了，可以放放，暂停对这个job的调度
			if ssn.JobReady(job) && !tasks.Empty() {  
				jobs.Push(job)
				break
			}
		}
		queues.Push(queue)  // Added Queue back until no job in Queue.
	}
}
```
`Session.Allocate` ==>  `if ssn.JobReady(job)  Session.dispatch`==>  `Cache.Bind(task *api.TaskInfo, hostname string) error` 真正 更新pod 即设定pod.nodeName。 
1. 对于jobReadyFns 来说，只有gang plugin 注册了jobReadyFns 到Session 上，Session.JobReady 默认返回true。也就是，如果不用gang plugin，则每一次 Session.Allocate，job ready默认为true，为pod 真正分配node。
2. 用了gang plugin之后，则每次Session.Allocate，要先校验 gang.jobReadyFns，校验通过，则为pod 真正分配node，否则只是将 task 标记为 api.Allocated ，记住了taskInfo 所属的nodeInfo，并在nodeInfo 中扣掉了taskInfo的资源`NodeInfo.addTask`。
3. 比如job一共10个 minMember=5，如果已经伪分配了4个，第5找不到合适的节点。gang plugin 向Session 注册了 ReclaimableFn/PreemptableFn（计算牺牲作业）。对于 这种状态的 job，这4个task 可以被抢占。

## Volcano

[华为官方文档](https://volcano.sh/zh/docs/)

[Volcano 在 Kubernetes 中运行高性能作业实践](https://time.geekbang.org/dailylesson/detail/100033217)

[一文带你解读Volcano架构设计与原理](https://segmentfault.com/a/1190000039122753)

### 整体设计

![](/public/upload/kubernetes/volcano_overview.png)

Volcano 支持`jobs.batch.volcano.sh.Job` workload，Volcano Controller 依据JobSpec创建依赖的Pod， Service， ConfigMap等资源，执行配置的插件，并负责Job后续的生命周期管理(状态监控，事件响应，资源清理等)。之后，Volcano Scheduler监听Pod资源的创建，依据策略，完成Pod资源的调度和绑定。

![](/public/upload/kubernetes/volcano_example_yml.jpeg)

### 使用

[使用Kubeflow和Volcano实现典型AI训练任务](https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0075.html)通过简单的设置schedulerName字段的值为“volcano”，启用Volcano调度器

```yml
kind: TFJob
metadata:
  name: {train_name}  
spec:
  schedulerName: volcano
  tfReplicaSpecs:
    Ps:
      replicas: {num_ps}
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          serviceAccount: default-editor
          containers:
          - name: tensorflow
            command:
            ...
            env:
            ...
            image: {image}
            workingDir: /opt
          restartPolicy: OnFailure
    Worker:
      replicas: 1
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          serviceAccount: default-editor
          containers:
          - name: tensorflow
            command:
            ...
            env:
            ...
            image: {image}
            workingDir: /opt
          restartPolicy: OnFailure
```

### 部分设计

1. job plugin:ssh提供免密登陆，svc提供作业运行所需要的网络信息如host文件、headless service等，来提供计算集群参数的自动化配置。
2. policies字段：一旦有pod被杀死，重启还是算了？RestartJob/CompeletedJob。因为这种HPC场景、基于Gang的训练作业，如果一个worker运行失败，通常来说整个作业运行下去是没什么意义的。
3. 没有podgroup的pod 不会被调度：webhook的功能主要是校验，其中一个校验是拦截create pod请求，如果根据pod 找不到podgroup则拒绝，确保所有podgroup 创建之后再创建pod。
4. volcano 中代码做了优化，将很多实际操作的逻辑从 session 剥离到 statment 中。statement 支持Evict/Pipeline/Allocate/Discard/Commit，关键就是这个Discard/Commit，前面几个操作（都有两个对象的小写方法，正反向操作，比如allocate/unallocate）被调用时并没有真正执行，而是一起丢弃或提交，**有点类似sql 中的事务**。

### action

```
// allocate 逻辑
// the allocation for pod may have many stages
// 1. pick a namespace named N (using ssn.NamespaceOrderFn)
// 2. pick a queue named Q from N (using ssn.QueueOrderFn)
// 3. pick a job named J from Q (using ssn.JobOrderFn)
// 4. pick a task T from J (using ssn.TaskOrderFn)
// 5. use predicateFn to filter out node that T can not be allocated on.
// 6. use ssn.NodeOrderFn to judge the best node and assign it to T
```

### plugin



```yml
actions: "enqueue, allocate, backfill"
tiers:
- plugins:
	- name: priority			# 让用户自定义job、task优先级
	- name: gang				# 满足了调度过程中的“All or nothing”的调度需求
	- name: conformance			# 除了 critical pod 都ok
- plugins:
	- name: drf					# 优先占用资源小的业务
	- name: predicates			# 快速筛选出来需要GPU的进行集中调度
	- name: proportion			# 如果投递的作业量超过所属queue最大可用资源，就需要排队。
	- name: nodeorder			# 从各个维度为node打分，打分参数由用户来配置。
	- name: binpack				# binpack调度算法的目标是尽量把已有的节点填满，每种资源配置的权重值、不同的插件在计算节点分数时权重值由用户来配置
```

为什么要分成tiers？背景：每个plugin 注册了一堆函数，action 会在会在适当的实际调用`Session.函数()`执行。`Session.函数()` 的大体逻辑都是 遍历plugin 注册的所有函数并执行，每个plugin 只注册了跟自己逻辑有关的函数。

1. priority, TaskOrderFn/JobOrderFn/PreemptableFn
2. gang, JobValidFn/ReclaimableFn/PreemptableFn/JobOrderFn/JobReadyFn/JobPipelinedFn/JobStarvingFns
3. conformance, PreemptableFn/ReclaimableFn
1. drf,  PreemptableFn/QueueOrderFn/ReclaimableFn/JobOrderFn/NamespaceOrderFn
3. predicates, PredicateFn
3. proportion, QueueOrderFn/ReclaimableFn/OverusedFn/UnderusedResourceFn/JobEnqueueableFn
4. nodeorder, NodeOrderFn/BatchNodeOrderFn
5. binpack , NodeOrderFn

`Session.函数()`  核心逻辑是两层循环，分为三种情况

1. “一言不合”直接返回的
	```go
	func (ssn *Session) xx() xx{
		for _, tier := range ssn.Tiers {
			for _, plugin := range tier.Plugins {
				if(不enable 这个方法 或者 没有这个方法){
					continue
				}
				if(xx){
					return xx
				}
			}
		}
		return xx
	}
	```
2. 所有plugin 一起配合计算的，比如给某个node 打分
	```go
	func (ssn *Session) xx() xx{
		for _, tier := range ssn.Tiers {
			for _, plugin := range tier.Plugins {
				求和
			}
		}
		return 和
	}
	```
3. tier内 的所有plugin 参与计算。比如 Reclaimable 决定回收哪些正在运行的pod，priority/gang/conformance 主要是用户自定义， 其它tier 则依据的是 task的实际情况， 比如用户定义的优先级很高，但违反了 proportion 规则的task 按照用户配置优先的原则进行调度。
	```go
	func (ssn *Session) xx() xx{
		tier 内结果
		for _, tier := range ssn.Tiers {
			for _, plugin := range tier.Plugins {
				聚合tier 内结果 
			}
			if (tier 内结果 有数据){
				retrun tier 内结果
			}
		}
		return nil
	
	```

前两种情况，是不需要区分两层的，此时所有的plugin 先后顺序是重要的，是不是在一个tier 里不重要，即要么立即结束要么全局聚合。第三种情况， tier内 的plugin **局部聚合**，两层for 之间做判断，如果有数据则 return。以示例配置文件来说，第一个tier 更多是基于用户设置，第二个tier 是基于task 和集群的实际情况，以用户设置为优先。

### 监控

[Volcano 监控设计解读](https://bbs.huaweicloud.com/blogs/detail/239635) 主要是延迟分布（有没有延迟，延迟多久），以及因为什么原因延迟。 

## Coscheduling

对于同一集群资源，调度器需要中心化。但如果同时存在两个调度器的话，有可能会出现决策冲突，例如分别将同一块资源分配给两个不同的Pod，导致某个Pod调度到节点后因为资源不足，导致无法创建的问题。解决的方式只能是通过标签的形式将节点强行的划分开来，或者部署多个集群。这种方式通过同一个Kubernetes集群来同时运行在线服务和离线作业，势必会导致整体集群资源的浪费以及运维成本的增加。再者，Volcano运行需要启动定制的MutatingAdmissionWebhook和ValidatingAdmissionWebhook。这些Webhooks本身存在单点风险，一旦出现故障，将影响集群内所有pod的创建。另外，多运行一套调度器，本身也会带来维护上的复杂性，以及与上游Kube-scheduler接口兼容上的不确定性。

[进击的 Kubernetes 调度系统（一）：Kubernetes scheduling framework](https://mp.weixin.qq.com/s/UkVXuZU0E0LT3LaDdZG4Xg)
[进击的Kubernetes调度系统（二）：支持批任务的Coscheduling/Gang scheduling](https://mp.weixin.qq.com/s/h_SzftCvyZeZO58Wgc1Hbg)
[进击的Kubernetes调度系统（三）：支持批任务的Binpack Scheduling](https://developer.aliyun.com/article/770336?spm=a2c6h.14164896.0.0.533961acjVVnOp) Kubernetes默认开启的资源调度策略是LeastRequestedPriority，消耗的资源最少的节点会优先被调度，使得整体集群的资源使用在所有节点之间分配地相对均匀。但是这种调度策略往往也会在单个节点上产生较多资源碎片。假设两个节点各剩余1GPU的资源，这时有申请2GPU的新作业，提交到调度器，则因为无法提供足够的资源，导致调度失败。每个节点都有1个GPU卡空闲，可是又无法被利用，导致资源GPU这种昂贵的资源被浪费。如果使用的资源调度策略是Binpack，优先将节点资源填满之后，再调度下一个节点，则资源碎片问题可以得到解决。

[网易有数机器学习平台批调度与k8s调度系统的深度解析](https://mp.weixin.qq.com/s/s-PecEMoLX-Gt5nfnibpwg)

## 资源调度的整体策略（未想好）

集群资源充足的情况就不说了，用李云龙团长的话说”这种富余仗，老子八辈子都没打过“。资源不足的具体矛盾是
1. 资源上的矛盾，你有100的资源，不代表都可以用，因为任务使用资源的粒度不是1。
2. 时间上的矛盾，有忙有闲，闲的时候也不说了，主要是忙的时候。
各个角色关心的部分
1. 管理员视角：最大吞吐量；优先级（任务的优先级别需要工程师申请报备）；最大集群资源利用率（减少资源碎片）
2. 用户视角：最大调度延迟；

假设排队等待的有j1/j2/j3，如果能够精确的预估每个任务的执行时间和实际耗费的资源，允许每个开发设置一个最晚开始时间（在这个时间开始之前等待成本为0），就可以按最小等待成本算一个提交序列。按最大集群资源利用率也可以算一个提交序列。 两个提交序列一样最好，不一样则优先最小等待时间。

[Volcano 监控设计解读](https://bbs.huaweicloud.com/blogs/detail/239635) 智能调度，根据“历史任务 的各方面指标 + 机器资源” 可以建立一个“预计耗时”的模型，进而估计一个大概的耗时时间。

考虑到部门因素，每个部门应该有一点自己的自留地，其它的集群资源按以上原则进行。
