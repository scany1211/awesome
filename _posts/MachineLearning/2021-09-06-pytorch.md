---

layout: post
title: pytorch
category: 架构
tags: MachineLearning
keywords:  pytorch

---

## 简介

* TOC
{:toc}


[这是我见过最好的NumPy图解教程！](https://mp.weixin.qq.com/s/77aq0JQs8SX6molSxewOdQ)NumPy是Python中用于数据分析、机器学习、科学计算的重要软件包。它极大地简化了向量和矩阵的操作及处理。《用python实现深度学习框架》用python 和numpy 实现了一个深度学习框架MatrixSlow
1. 支持计算图的搭建、前向、反向传播
2. 支持多种不同类型的节点，包括矩阵乘法、加法、数乘、卷积、池化等，还有若干激活函数和损失函数。比如向量操作pytorch 跟numpy 函数命名都几乎一样
3. 提供了一些辅助类和工具函数，例如构造全连接层、卷积层和池化层的函数、各种优化器类、单机和分布式训练器类等，为搭建和训练模型提供便利

## 什么是pytorch
[Pytorch详细介绍(上)](https://mp.weixin.qq.com/s/YDNX49NGWMqkLML0_k0Phg)Pytorch是一个很容易学习、使用和部署的深度学习库。PyTorch同时提供了更高性能的C++ API（libtorch）。Pytorch使用Tensor作为核心数据结构，Tensor（张量）是神经网络世界的Numpy，其类似与Numpy数组，但是又不同于Numpy数组。为了提高Tensor的计算速度，有大量的硬件和软件为了Tensor运算进行支持和优化。总得来说，Tensor具有如下特点：

1. 具有类似Numpy的数据，Tensor可以与Numpy共享内存；
2. 可以指定计算设备，例如，设定Tensor是在CPU上计算，还是在GPU上计算；
3. Tensor可微分。

PyTorch工作流以及与每个步骤相关联的重要模块

![](/public/upload/machine/pytorch_process.png)

## 张量/Tensor

在深度学习过程中最多使用`$5$`个维度的张量，张量的维度（dimension）通常叫作轴（axis）
1. 标量（0维张量）
2. 向量（1维度张量）
3. 矩阵（2维张量）
4. 3维张量，最常见的三维张量就是图片，例如`$[224, 224, 3]$`
4. 4维张量，4维张量最常见的例子就是批图像，加载一批 `$[64, 224, 224, 3]$` 的图片，其中 `$64$` 表示批尺寸，`$[224, 224, 3]$` 表示图片的尺寸。
5. 5维张量，使用5维度张量的例子是视频数据。视频数据可以划分为片段，一个片段又包含很多张图片。例如，`$[32, 30, 224, 224, 3]$` 表示有 `$32$` 个视频片段，每个视频片段包含 `$30$` 张图片，每张图片的尺寸为 `$[224, 224, 3]$`。

本质上来说，**PyTorch 是一个处理张量的库**。
1. Tensor的结构操作包括：创建张量，查看属性，修改形状，指定设备，数据转换， 索引切片，广播机制，元素操作，归并操作；
2. Tensor的数学运算包括：标量运算，向量运算，矩阵操作，比较操作。

张量有很多属性，下面我们看看常用的属性有哪些？

1. tensor.shape，tensor.size(): 返回张量的形状；
2. tensor.ndim：查看张量的维度；
3. tensor.dtype，tensor.type()：查看张量的数据类型；
4. tensor.is_cuda：查看张量是否在GPU上；
5. tensor.grad：查看张量的梯度；
6. tensor.requires_grad：查看张量是否可微。


使用gpu
```python
cpu = torch.device("cpu")
gpu = torch.device("cuda:0")  # 使用第一个cpu
x = torch.rand(10)
x = x.to(gpu)
```

梯度
```python
x = torch.tensor(3.)
w = torch.tensor(4., requires_grad=True) # 是否要求计算 y 相对 w 的梯度
b = torch.tensor(5., requires_grad=True) # 是否要求计算 y 相对 b 的梯度
y = w * x + b
print y
# tensor(17. grad_fn=<AddBackward0>)
y.backward()
print('dy/dx:', x.grad)
# dy/dx: Node
print('dy/dw:', w.grad)
# dy/dx: tensor(3.)
print('dy/db:', b.grad)
# dy/dx: tensor(1.)
```
我们已经创建了 3 个张量：x、w 和 b, y 是值为 3 * 4 + 5 = 17 的张量

## 训练
[Pytorch分布式训练](https://mp.weixin.qq.com/s/G-uLl3HXzFJOW03nA7etig)
单机单卡训练步骤：定义网络，定义dataloader，定义loss和optimizer，开训

```python
BATCH_SIZE = 256
EPOCHS = 5
if __name__ == "__main__":
    # 1. define network
    device = "cuda"
    net = torchvision.models.resnet18(num_classes=10)
    net = net.to(device=device)  
    # 2. define dataloader
    trainset = torchvision.datasets.CIFAR10(...)
    train_loader = torch.utils.data.DataLoader(
        trainset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    # 3. define loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(
        net.parameters(),
        lr=0.01,
        momentum=0.9,
        weight_decay=0.0001,
        nesterov=True,
    )
    print("            =======  Training  ======= \n")
    # 4. start to train
    net.train()
    for ep in range(1, EPOCHS + 1):
        train_loss = correct = total = 0

        for idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)

            loss = criterion(outputs, targets)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            total += targets.size(0)
            correct += torch.eq(outputs.argmax(dim=1), targets).sum().item()

             # 输出loss 和 正确率
    print("\n            =======  Training Finished  ======= \n")
```
如果是单机多卡，定义网络时加入 `net = nn.DataParallel(net)`

[DDP系列第一篇：入门教程](https://zhuanlan.zhihu.com/p/178402798)多机多卡 
1. 分布式训练的几个选择：模型并行 vs 数据并行，PS 模型 vs Ring-Allreduce ，pytorch 单机多卡（DP）用PS 模型，多机多卡（DDP）用Ring-Allreduce
2. 假设 有两台机子，每台8张显卡，那就是2x8=16个进程，并行数是16。DDP 推荐每个进程一张卡，在16张显卡，16的并行数下，DDP会同时启动16个进程。rank表示当前进程的序号（0~16），用于进程间通讯。local_rank 表示每台机子上的进程的序号（0~7）。用起来 就是一行代码，`model = DDP(model, device_ids=[local_rank], output_device=local_rank)`，**后续的模型关于前向传播、后向传播的用法，和单机单卡完全一致**。[DDP系列第二篇：实现原理与源代码解析](https://zhuanlan.zhihu.com/p/187610959)
3. 每个进程跑的是同一份代码，进程从外界（比如环境变量）获取 rank/master_addr/master_port 等参数，rank = 0 的进程为 master 进程


```python
BATCH_SIZE = 256
EPOCHS = 5
if __name__ == "__main__":
    # 0. set up distributed device
    rank = int(os.environ["RANK"])
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(rank % torch.cuda.device_count())
    dist.init_process_group(backend="nccl")
    device = torch.device("cuda", local_rank) # 把模型放置到特定的GPU上
    print(f"[init] == local rank: {local_rank}, global rank: {rank} ==")
    # 1. define network
    net = torchvision.models.resnet18(pretrained=False, num_classes=10)
    net = net.to(device)
    # DistributedDataParallel
    net = DDP(net, device_ids=[local_rank], output_device=local_rank)
    # 2. define dataloader
    trainset = torchvision.datasets.CIFAR10(...)
    # DistributedSampler
    # we test single Machine with 2 GPUs so the [batch size] for each process is 256 / 2 = 128
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        trainset,
        shuffle=True,
    )
    train_loader = torch.utils.data.DataLoader(
        trainset,
        batch_size=BATCH_SIZE,
        num_workers=4,
        pin_memory=True,
        sampler=train_sampler,
    )
    # 3. define loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(
        net.parameters(),
        lr=0.01 * 2,
        momentum=0.9,
        weight_decay=0.0001,
        nesterov=True,
    )
    if rank == 0:
        print("            =======  Training  ======= \n")
    # 4. start to train
    net.train()
    for ep in range(1, EPOCHS + 1):
        train_loss = correct = total = 0
        # set sampler
        train_loader.sampler.set_epoch(ep)

        for idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)

            loss = criterion(outputs, targets)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            total += targets.size(0)
            correct += torch.eq(outputs.argmax(dim=1), targets).sum().item()
            # 输出loss 和 正确率
    if rank == 0:
        print("\n            =======  Training Finished  ======= \n")
```




## 部署

[如何部署 PyTorch 模型](https://zhuanlan.zhihu.com/p/344364948) TorchServe